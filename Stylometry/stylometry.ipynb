{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributing Authorship w/ Stylometry\n",
    "<i>Stylometry</i> is the quantitative study of literary style through computational text analysis. It's based on the idea that we all have unique, consistent, and recognizable style to our writing. This includes our vocabulary, our use of punctuation, the average length of our sentences and words, and so on.\n",
    "\n",
    "Stylometry has been used to overturn murder convictions and even helped identify and convict the Unabomber in 1996. Other uses include detecting plagiarism and determining the emotional tone behind words, such as in social media posts.\n",
    "\n",
    "### The Objective\n",
    "Write a Python program that uses stylometry to determine whether Sir Arthour Conan Doyle or H. G. Wells wrote the novel <i>The Lost World</i>.\n",
    "\n",
    "### The Strategy\n",
    "The science of <i>natural language processing</i> (NLP) deals with the interactions between the precise and structured language of computers and the nuanced freqhently ambiguous \"natural\" language used by humans. Example uses for NLP include machine translations, spam detection, comprehension of search engine questions, and predictive text recognition for cell phone users.\n",
    "The most common NLP tests for authorship analyze the following features of a text:\n",
    "    <b>Word Length</b>: a frequency distribution plot of the length of words in a document\n",
    "    <b>Stop Words</b>: a frequency distribution plot of stop words (short, noncontextual function words like \"the\", \"but\", and \"if\")\n",
    "    <b>Parts of Speech</b>: a frequency distribution plot of words based on their syntactic functions (such as nouns, pronouns, verbs, adverbs, adjectives, and so on)\n",
    "    <b>Most Common Words</b>: a comparison of the most commonly used words in a text\n",
    "    <b>Jaccard Similarity</b>: a statistic used for gauging the similarity and diversity of a sample set\n",
    "If Doyle and Wells have distinctive writing styles, these five tests should be enough to distinguish between them.\n",
    "To capture and analyze each author's style, you'll need a representtive <i>corpus</i>, or a body of text. FOr Dyle, use the famous Sherlock Holmes novel <i>The Hound of the Baskervilles</i>, published in 1902. For Wells, use <i>The War of the Worlds</i>, published in 1898.\n",
    "\n",
    "Both these novels contain more than 50,000 words, more than enough for a sound statistical sampling. You'll then compare each author's sample to <i>The Lost World</i> to determine how closely the writing styles match.\n",
    "\n",
    "To perform stylometry, we'll use the <i>National Language Toolkit</i> (NLTK), a popular suite of programs and libraries for working with human language data in Python. Created in 2001 as part of a computational linguistics course at the University of Pennsylvania, NLTK has continued to develop and expand with the help of dozens of contributors. To learn more, check out the official NLTK website at http://www.nltk.org/.\n",
    "\n",
    "### Downloading the Tokenizer\n",
    "To run the stylometric tests, you'll need to break the multiple texts - or corpora - into individualized words, referred to as tokens.\n",
    "The `word_tokenize()` method in NLTK implicitly calls `sent_tokenize()`, `tokenize()`, you'll need the Punkt Tokenizer Models. Although this is part of NLTK, you'll have to download it separately with the handy NLTK Downloader. To launch it, enter the following into the Python shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK Downloader window should now be open. Click either the <b>Models</b> or <b>All Packages</b> tab near the top, then click <b> punkt</b> in the Identifier column. Scroll to the bottom of the window and set the Download Directory for your platform. Finally, click the <b>Download</b> button to download the Punkt Tokenizer Models.\n",
    "Note that you can also download NLTK packages directly in the shell. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dylan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Stopwords Corpus\n",
    "Click the <b>Corpora</b> tab in the NLTK Downloader window and download the Stopwords Corpus.\n",
    "We should download one more package to help us analyze parts of speech, like nowns and verbs. Click the <b>All Packages</b> tab in the NLTK Downloader window and download the Averaged Perceptron Tagger."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Corpora\n",
    "You can download the text files for <i>The Hound of the Baskervilles</i> (hound.txt), <i>The War of the Worlds</i> (war.txt), and <i>The Lost World</i> (lost.txt), along with the books code, from the GitHub repo with this file.\n",
    "\n",
    "These came from Project Gutenberg (`http://www.gutenberg.org/`), a great source for public domain literature. So that you can use these texts right away, we've stripped them of extraneous material such as table of contents, chapter titles, copyright information, and so on.\n",
    "\n",
    "### The Stylometry Code\n",
    "The <i>stylometry.py</i> program you'll write next loads the text files as strings, tokenizes them into words, and then runs the five stylometric analyses listed above. The program will output a combination of plots and shell messages that will help you determine who wrote <i>The Lost World</i>.\n",
    "Make sure to keep the program in the same folder as the three text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LINES = ['-', ':', '--'] #Line styles for plots\n",
    "\n",
    "def main():\n",
    "    strings_by_author = dict()\n",
    "    strings_by_author['doyle'] = text_to_string('hound.txt')\n",
    "    strings_by_author['wells'] = text_to_string('war.txt')\n",
    "    strings_by_author['unknown'] = text_to_string('lost.txt')\n",
    "\n",
    "    print(strings_by_author['doyle'][:300])\n",
    "\n",
    "    words_by_author = make_word_dict(strings_by_author)\n",
    "    len_shortest_corpus = find_shortest_corpus(words_by_author)\n",
    "    word_length_test(words_by_author, len_shortest_corpus)\n",
    "    stopwords_test(words_by_author, len_shortest_corpus)\n",
    "    parts_of_speech_test(words_by_author, lent_shortest_corpus)\n",
    "    vocab_test(words_by_author)\n",
    "    jaccard_test(words_by_author, len_shortest_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Text and Building a Word Dictionary\n",
    "The next block of text defines two functions. The first reads in a text file as a string. The second builds a dictionary with each author's name as the key and his novel, now tokenized into individual words rather than a continuous string as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_string(filename):\n",
    "    #Read a text file and return a string\n",
    "    with open(filename) as infile:\n",
    "        return infile.read()\n",
    "\n",
    "def make_word_dict(strings_by_author):\n",
    "    #Return dictioniary of tokenized words by corpus by author\n",
    "    words_by_author = dict()\n",
    "    for author in strings_by_author:\n",
    "        tokens = nltk.word_tokenize(strings_by_author[author])\n",
    "        words_by_author[author] = ([token.lower() for token in tokens if token.isalpha()])\n",
    "    return words_by_author"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some users may encounter a UnicodeDecodeError like the following one when loading the text:\n",
    "\n",
    "`UnicodeDecodeError: 'ascii' codec can't decode byte 0x93 in position 365: ordinal not in range(128)`\n",
    "\n",
    "Encoding and decoding refer to the process of converting from characters stored as bytes to human-readable strings. The problem is that the default encoding forthe built-in function `open()` is platform dependent and depends on the value of `locale.getpreferredencoding()`.\n",
    "\n",
    "### Finding the Shortest Corpus\n",
    "In computational linguistics, frequency refers to the number of occurrences in a corpus. Thus, frequency means the count, and methods you'll use later return a dictionary of words and their counts. To compare counts in a meaningful way, the corpora should all have the same number of words. Because the three corpora used here are large, you can safely normalize the corpora by truncating them all to the length of the shortest. The code block below defines a function that finds the shortest corpus in the `words_by_author` dictionary and returns its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shortest_corpus(words_by_author):\n",
    "    #Return length of shortest corpus\n",
    "    word_count = []\n",
    "    for author in words_by_author:\n",
    "        word_count.append(len(words_by_author[author]))\n",
    "        print('\\nNumber of words for {} = {}\\n'.format(author, len(words_by_author[author])))\n",
    "        len_shortest_corpus = min(word_count)\n",
    "        print('length shortest corpus = {}\\n'.format(len_shortest_corpus))\n",
    "        return len_shortest_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Word Lengths\n",
    "Part of an author's distinctive style is the words they use. Faulkner observed that Hemingway never sent a reader running to the dictionary; Hemingway accused Faulkner of using \"10-dollar words.\" Authorial style is expressed in the length of words and in vocabulary, which we'll look at later in the chapter.\n",
    "\n",
    "The code block below defines a function to compare the length of words per corpus and plot the results as a frequency distribution. In a frequency distribution, the lengths of words are plotted against the number of counts for each length. For words that are six letters long, for example, one author may have a count of 4,000, and another may have a count of 5,500. A frequency distribution allows comparison across a range of word lengths, rather than just at the average word length.\n",
    "\n",
    "The code block below uses list slicing to truncate the word lists to the length of the shortest corpus so the results aren't skewed by the size of the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_length_test(words_by_author, len_shortest_corpus):\n",
    "    #Plot word length frequency by author, truncated to shortest corpus length\n",
    "    by_author_length_freq_dist = dict()\n",
    "    plt.figure(1)\n",
    "    plt.ion()\n",
    "\n",
    "    for i, author in enumerate(words_by_author):\n",
    "        word_lengths = [len(word) for word in words_by_author[author][:len_shortest_corpus]]\n",
    "        by_author_length_freq_dist[author] = nltk.FreqDist(word_lengths)\n",
    "        by_author_length_freq_dist[author].plot(15, linestyle=LINES[i], label=author, title='Word Length')\n",
    "        plt.legend()\n",
    "        plt.show() #Uncomment to see plot while coding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Stop Words\n",
    "A stop word is a small word used often, like \"the\", \"by\", and \"but\". These words are filtered out for tasks like online searches, because they provide no contextual information, and they were once thought to be of little value in identifying authorship.\n",
    "\n",
    "But stop words, used frequently and without much thought, are perhaps the best signature for an author's style. And since the text you're comparing are usually about different subjects, these stop words become important, as they are agnostic to content and common across all texts.\n",
    "\n",
    "The text block below defines a function to compare the use of stop words in the three corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_test(words_by_author, lent_shortest_corpus):\n",
    "    #Plot stopwords freq by author, truncated to shortest corpus length\n",
    "    stopwords_by_author_freq_dist = dict()\n",
    "    plt.figure(2)\n",
    "    stop_words = set(stopwords.words('english')) #Use set for speed\n",
    "    #print('Number of stopwords = {}\\n'.format(len(stop_words)))\n",
    "    #print('Stopwords = {}\\n'.format(stop_words))\n",
    "\n",
    "    for i, author in enumerate(words_by_author):\n",
    "        stopwords_by_author = [word for word in words_by_author[author][:len_shortest_corpus] if word in stop_words]\n",
    "        stopwords_by_author_freq_dist[author] = nltk.FreqDist(stopwords_by_author)\n",
    "        stopwords_by_author_freq_dist[author].plot(50, label=author, linestyle=LINES[i], title='50 Most Common Stopwords')\n",
    "\n",
    "    plt.legend()\n",
    "    ##plt.show() #Uncomment to see plot while coding function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Parts of Speech\n",
    "Now let's compare the parts of speech used in the three corpora. NLTK uses a part-of-speech (POS) tagger, called `PerceptronTagger`, to identify parts of speech. POS taggers process a sequence of tokenized words and attach a POS tag to each word.\n",
    "\n",
    "The taggers are typically trained on large datasets like the <i>Penn Treebank</i> or <i>Brown Corpus</i>, making thm highly accurate, though not perfect. You can also find training data and taggers forlanguages other than English. You don't need to worry about all these various terms and their abbreviations. As with previous tests, you'll just need to compare lines in a chart. The code block below defines a function to plot the frequency distribution of POS in the three corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_of_speech_test(words_by_author, len_shortest_corpus):\n",
    "    #Plot author use of parts of speech such as nouns, verbs, adverbs, etc\n",
    "    by_author_pos_freq_dist = dict()\n",
    "    plt.figure(3)\n",
    "    for i, author in enumerate(words_by_author):\n",
    "        pos_by_author = [pos[1] for pos in nltk.pos_tag(words_by_author[author][:len_shortest_corpus])]\n",
    "        by_author_pos_freq_dist[author] = nltk.FreqDist(pos_by_author)\n",
    "        by_author_pos_freq_dist[author].plot(35, label=author, linestyle=LINES[i], title='Part of Speech')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Author Vocabularies\n",
    "To compare the vocabularies among the three corpora, you'll use the chi-squared random variable (X<sup>2</sup>), also known as the <i>test statistic</i>, to measure the \"distance\" between the vocabularies employed in the unknown corpus and each of the known corpora. The closest vocabularies will be the most similar. The formula is:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Where <i>O</i> is the observed word count and <i>E</i> is the expected word count assuming the corpora being compared are both by the same author. If Dyle wrote both novbels, they should both have the same, or a similar, proportion of the most common words. The test statistic lets you quantify how similar they are by measuring how much the counts for each word differ. The lower the chi-squared test statistic, the greater the similarity between two distributions.\n",
    "\n",
    "The code block below defines a function to compare vocabularies among the three corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_test(words_by_author):\n",
    "    #Compare author vocabularies using the chi-squared statistical test\n",
    "    chisquared_by_author = dict()\n",
    "    for author in words_by_author:\n",
    "        if author != 'unknown':\n",
    "            combined_corpus = (words_by_author[author] + words_by_author['unknown'])\n",
    "            author_proportion = (len(words_by_author[author]) / len(combined_corpus))\n",
    "            combined_freq_dist = nltk.FreqDist(combined_corpus)\n",
    "            most_common_words = list(combined_freq_dist.most_common(1000))\n",
    "            chisquared = 0\n",
    "            for word, combined_count in most_common_words:\n",
    "                observed_count_author = words_by_author[author].count(word)\n",
    "                expected_count_author = combined_count * author_proportion\n",
    "                chisquared += ((observed_count_author - expected_count_author) ** 2 / expected_count_author)\n",
    "                chisquared_by_author[author] = chisquared\n",
    "            print('Chi-squared for {} = {:.1f}'.format(author,chisquared))\n",
    "    most_likely_author = min(chisquared_by_author, key=chisquared_by_author.get)\n",
    "    print('Most-likely author by vocabulary is {}\\n'.format(most_likely_author))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Jaccard Similarity\n",
    "To determine the degree of similarity among sets created from the corpora, we'll use the <i>Jaccard similarity coefficient</i>. Also called the \"intersection over union\", this is simply the area of overlap between two sets divided by the area of the union of the two sets.\n",
    "\n",
    "The more overlap there is between sets created from two texts, the more likely they were written by the same author.\n",
    "The code block below defines a function for gauging the similarity of sample sets, and makes sure the main() program is run when it is the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_test(words_by_author, len_shortest_corpus):\n",
    "    #Calculate Jaccard similarity fo eah known corpus to unknown corpus\n",
    "    jaccard_by_author = dict()\n",
    "    unique_words_unknown = set(words_by_author['unknown'][:len_shortest_corpus])\n",
    "    authors = (author for author in words_by_author if author != 'unknown')\n",
    "    for author in authors:\n",
    "        unique_words_author = set(words_by_author[author][:len_shortest_corpus])\n",
    "        shared_words = unique_words_author.intersection(unique_words_unknown)\n",
    "        jaccard_sim = (float(len(shared_words)) / (len(uniqueA_words_author) + len(unique_words_unknown) - len(shared_words)))\n",
    "        jaccard_by_author[author] = jaccard_sim\n",
    "        print('Jaccard Similarity for {} = {}'.format(author, jaccard_sim))\n",
    "    most_likely_author = max(jaccard_by_author, key=jaccard_by_author.get)\n",
    "    print('Most-likely author by similarity is {}'.format(most_likely_author))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "841b185554555ca1ca178be24c38857c2b72fe74f8a7d4209f77b3f65f0ea57b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
