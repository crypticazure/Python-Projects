{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributing Authorship w/ Stylometry\n",
    "<i>Stylometry</i> is the quantitative study of literary style through computational text analysis. It's based on the idea that we all have unique, consistent, and recognizable style to our writing. This includes our vocabulary, our use of punctuation, the average length of our sentences and words, and so on.\n",
    "\n",
    "Stylometry has been used to overturn murder convictions and even helped identify and convict the Unabomber in 1996. Other uses include detecting plagiarism and determining the emotional tone behind words, such as in social media posts.\n",
    "\n",
    "### The Objective\n",
    "Write a Python program that uses stylometry to determine whether Sir Arthour Conan Doyle or H. G. Wells wrote the novel <i>The Lost World</i>.\n",
    "\n",
    "### The Strategy\n",
    "The science of <i>natural language processing</i> (NLP) deals with the interactions between the precise and structured language of computers and the nuanced freqhently ambiguous \"natural\" language used by humans. Example uses for NLP include machine translations, spam detection, comprehension of search engine questions, and predictive text recognition for cell phone users.\n",
    "The most common NLP tests for authorship analyze the following features of a text:\n",
    "    <b>Word Length</b>: a frequency distribution plot of the length of words in a document\n",
    "    <b>Stop Words</b>: a frequency distribution plot of stop words (short, noncontextual function words like \"the\", \"but\", and \"if\")\n",
    "    <b>Parts of Speech</b>: a frequency distribution plot of words based on their syntactic functions (such as nouns, pronouns, verbs, adverbs, adjectives, and so on)\n",
    "    <b>Most Common Words</b>: a comparison of the most commonly used words in a text\n",
    "    <b>Jaccard Similarity</b>: a statistic used for gauging the similarity and diversity of a sample set\n",
    "If Doyle and Wells have distinctive writing styles, these five tests should be enough to distinguish between them.\n",
    "To capture and analyze each author's style, you'll need a representtive <i>corpus</i>, or a body of text. FOr Dyle, use the famous Sherlock Holmes novel <i>The Hound of the Baskervilles</i>, published in 1902. For Wells, use <i>The War of the Worlds</i>, published in 1898.\n",
    "\n",
    "Both these novels contain more than 50,000 words, more than enough for a sound statistical sampling. You'll then compare each author's sample to <i>The Lost World</i> to determine how closely the writing styles match.\n",
    "\n",
    "To perform stylometry, we'll use the <i>National Language Toolkit</i> (NLTK), a popular suite of programs and libraries for working with human language data in Python. Created in 2001 as part of a computational linguistics course at the University of Pennsylvania, NLTK has continued to develop and expand with the help of dozens of contributors. To learn more, check out the official NLTK website at http://www.nltk.org/.\n",
    "\n",
    "### Downloading the Tokenizer\n",
    "To run the stylometric tests, you'll need to break the multiple texts - or corpora - into individualized words, referred to as tokens.\n",
    "The `word_tokenize()` method in NLTK implicitly calls `sent_tokenize()`, `tokenize()`, you'll need the Punkt Tokenizer Models. Although this is part of NLTK, you'll have to download it separately with the handy NLTK Downloader. To launch it, enter the following into the Python shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK Downloader window should now be open. Click either the <b>Models</b> or <b>All Packages</b> tab near the top, then click <b> punkt</b> in the Identifier column. Scroll to the bottom of the window and set the Download Directory for your platform. Finally, click the <b>Download</b> button to download the Punkt Tokenizer Models.\n",
    "Note that you can also download NLTK packages directly in the shell. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Stopwords Corpus\n",
    "Click the <b>Corpora</b> tab in the NLTK Downloader window and download the Stopwords Corpus.\n",
    "We should download one more package to help us analyze parts of speech, like nowns and verbs. Click the <b>All Packages</b> tab in the NLTK Downloader window and download the Averaged Perceptron Tagger."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Corpora\n",
    "You can download the text files for <i>The Hound of the Baskervilles</i> (hound.txt), <i>The War of the Worlds</i> (war.txt), and <i>The Lost World</i> (lost.txt), along with the books code, from the GitHub repo with this file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "841b185554555ca1ca178be24c38857c2b72fe74f8a7d4209f77b3f65f0ea57b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
